<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN">
<html><head>
  <title>DSL Lecture 8</title>

  
  
  <link rel="stylesheet" href="PS_Lecture.css">

</head><body>
<ul class="navbar">

  <li><a href="PS_DSL_Lecture_01.html">Lecture 1</a></li>
  <li><a href="PS_DSL_Lecture_02.html">Lecture 2</a></li>
  <li><a href="PS_DSL_Lecture_03.html">Lecture 3</a></li>
  <li><a href="PS_DSL_Lecture_04.html">Lecture 4</a></li>
  <li><a href="PS_DSL_Lecture_05.html">Lecture 5</a></li>
  <li><a href="PS_DSL_Lecture_06.html">Lecture 6</a></li>
  <li><a href="PS_DSL_Lecture_07.html">Lecture 7</a></li>
  <li><a href="PS_DSL_Lecture_08.html">Lecture 8</a></li>
  <li><a href="PS_DSL_Lecture_09.html">Lecture 9</a></li>
  <li><a href="PS_DSL_Lecture_10.html">Lecture 10</a></li>
  <li><a href="PS_DSL_Lecture_11.html">Lecture 11</a></li>
  <li><a href="PS_DSL_Lecture_12.html">Lecture 12</a></li>
</ul>

<!-- Main content -->
<h1>Penn State Great Valley<br>
DSL Course - Lecture 8<br>
</h1>

<hr style="width: 100%; height: 2px;">
<h1>Getting to BNF + Parser Generators &amp; Parser
Combinators</h1>

<h1><img style="width: 944px; height: 297px;" alt="Aurora" src="../../../../_work/PennState/DSL/Lecture/images/aurora/aurora/AuroraPrelude_takasaka600h.jpg"><br>
</h1>

<h2>Getting to BNF</h2>

<h3>What is BNF?</h3>

<p><br>
Backus-Naur notation (more commonly
known as BNF or Backus-Naur Form) is a formal mathematical way to
describe a language, which was developed by John Backus (and possibly
Peter Naur as well) to describe the syntax of the Algol 60 programming
language.<br>
<br>
(Legend has it that it was primarily developed by John
Backus (based on earlier work by the mathematician Emil Post), but
adopted and slightly improved by Peter Naur for Algol 60, which made it
well-known. Because of this Naur calls BNF Backus Normal Form, while
everyone else calls it Backus-Naur Form.)<br>
<br>
It is used to formally
define the grammar of a language, so that there is no disagreement or
ambiguity as to what is allowed and what is not. In fact, BNF is so
unambiguous that there is a lot of mathematical theory around these
kinds of grammars, and one can actually mechanically construct a parser
for a language given a BNF grammar for it. (There are some kinds of
grammars for which this isn't possible, but they can usually be
transformed manually into ones that can be used.)<br>
<br>
Programs that do this are commonly called "compiler compilers" or
Parser Generators (see below).&nbsp;</p>

<h3>How it works</h3>

<h4>The principles</h4>

<p>BNF
is sort of like a mathematical game: you start with a symbol (called
the start symbol and by convention usually named S in examples) and are
then given rules for what you can replace this symbol with. The
language defined by the BNF grammar is just the set of all strings you
can produce by following these rules.<br>
<br>
The rules are called production rules, and look like this:<br>
<br>
</p>

<div class="code">&nbsp; symbol := alternative1 | alternative2 ...<br>
</div>

<br>

A
production rule simply states that the symbol on the left-hand side of
the := must be replaced by one of the alternatives on the right hand
side. The alternatives are separated by |s. (One variation on this is
to use ::= instead of :=, but the meaning is the same.) Alternatives
usually consist of both symbols and something called terminals.
Terminals are simply pieces of the final string that are not symbols.
They are called terminals because there are no production rules for
them: they terminate the production process. (Symbols are often called
non-terminals.)<br>

<br>

Another variation on BNF grammars is to enclose
terminals in quotes to distinguish them from symbols. Some BNF grammars
explicitly show where whitespace is allowed by having a symbol for it,
while other grammars leave this for the reader to infer.<br>

<br>

There
is one special symbol in BNF: @, which simply means that the symbol can
be removed. If you replace a symbol by @, you do it by just removing
the symbol. This is useful because in some cases it is difficult to end
the replacement process without using this trick.<br>

<br>

So, the
language described by a grammar is the set of all strings you can
produce with the production rules. If a string cannot in any way be
produced by using the rules the string is not allowed in the language.
<h3>The BNF Highlightes Again</h3>

<p>The meta-symbols of BNF are:<br>
<br>
::=&nbsp;&nbsp;&nbsp; meaning "is defined as" <br>
|&nbsp;&nbsp; &nbsp; &nbsp;&nbsp; meaning "or" <br>
&lt; &gt;&nbsp;&nbsp;&nbsp; angle brackets used to
surround category names. <br>
<br>
The angle brackets distinguish syntax rules names (also called
non-terminal symbols) from terminal symbols which are written exactly
as they are to be represented. A BNF rule defining a nonterminal has
the form:<br>
<br>
</p>

<div class="code">nonterminal ::= sequence_of_alternatives consisting
of strings of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
terminals or nonterminals separated by the meta-symbol |<br>
</div>

<br>

For example, the BNF production for a mini-language is:<br>

<br>

<div class="code">&lt;program&gt; ::=&nbsp; program<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&lt;declaration_sequence&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
begin<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&lt;statements_sequence&gt;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
end ;<br>
</div>

<br>

This shows that a mini-language program consists of the keyword
"program" followed by the declaration sequence, then the keyword
"begin" and the statements sequence, finally the keyword "end" and a
semicolon.
<p><br>
</p>

<h3>A real example</h3>

Below is a sample BNF grammar:<br>

<p>
</p>

<div class="code"><br>
&nbsp; S&nbsp; := '-' FN |<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; FN<br>
&nbsp; FN := DL |<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DL '.' DL<br>
&nbsp; DL := D |<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; D DL<br>
&nbsp; D&nbsp; := '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' |
'9'<br>
<br>
</div>

<br>

The
different symbols here are all abbreviations: S is the start symbol, FN
produces a fractional number, DL is a digit list, while D is a digit.<br>

<br>

Valid
sentences in the language described by this grammar are all numbers,
possibly fractional, and possibly negative. To produce a number, start
with the start symbol S:<br>

<br>

&nbsp; S<br>

<br>

Then replace the S
symbol with one of its productions. In this case we choose not to put a
'-' in front of the number, so we use the plain FN production and
replace S by FN:<br>

<br>

&nbsp; FN<br>

<br>

The next step is then to
replace the FN symbol with one of its productions. We want a fractional
number, so we choose the production that creates two decimal lists with
a '.' between them, and after that we keep choosing replacing a symbol
with one of its productions once per line in the example below:<br>

<br>

&nbsp; DL . DL<br>

<br>

&nbsp; D . DL<br>

<br>

&nbsp; 3 . DL<br>

<br>

&nbsp; 3 . D DL<br>

<br>

&nbsp; 3 . D D<br>

<br>

&nbsp; 3 . 1 D<br>

<br>

&nbsp; 3 . 1 4<br>

<br>

Here
we've produced the fractional number 3.14. How to produce the number -5
is left as an exercise for the reader. To make sure you understand this
you should also study the grammar until you understand why the string
3..14 cannot be produced with these production rules.
<h3>BNF Defined in BNF</h3>

<p>Now as a last example (maybe not the easiest to read !), here is the
definition of BNF expressed in BNF:<br>
</p>

<div class="code"><br>
syntax&nbsp;&nbsp;&nbsp;&nbsp; ::=&nbsp; { rule }<br>
rule&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
::=&nbsp; identifier&nbsp; "::="&nbsp; expression<br>
expression ::=&nbsp; term { "|" term }<br>
term&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
::=&nbsp; factor { factor }<br>
factor&nbsp;&nbsp;&nbsp;&nbsp; ::=&nbsp; identifier
|<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
quoted_symbol |<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
"("&nbsp; expression&nbsp; ")" |<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
"["&nbsp; expression&nbsp; "]" |<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
"{"&nbsp; expression&nbsp; "}"<br>
identifier ::=&nbsp; letter { letter | digit }<br>
quoted_symbol ::= """ { any_character } """<br>
<br>
</div>

<h3>EBNF: What is it, and why do we need it?</h3>

<p>In
DL I had to use recursion (ie: DL can produce new DLs) to express the
fact that there can be any number of Ds. This is a bit awkward and
makes the BNF harder to read. Extended BNF (EBNF, of course) solves
this problem by adding three operators:<br>
<br>
&nbsp;&nbsp;&nbsp; ? :
which means that the symbol (or group of symbols in parenthesis) to the
left of the operator is optional (it can appear zero or one times)<br>
&nbsp;&nbsp;&nbsp; * : which means that something can be repeated any
number of times (and possibly be skipped altogether)<br>
&nbsp;&nbsp;&nbsp; + : which means that something can appear one or
more times<br>
<br>
</p>

<h3>An EBNF sample grammar</h3>

<p>So in extended BNF the above grammar can be written as:<br>
</p>

<div class="code"><br>
&nbsp; S := '-'? D+ ('.' D+)?<br>
<br>
&nbsp; D := '0' | '1' | '2' | '3' | '4' | '5' | '6' | '7' | '8' | '9'<br>
<br>
</div>

<br>

which is rather nicer. :)<br>

<br>

Just
for the record: EBNF is not more powerful than BNF in terms of what
languages it can define, just more convenient. Any EBNF production can
be translated into an equivalent set of BNF productions.<br>

<br>

<h2>Parser Generators</h2>

<p>So now you have your grammar file, the natural way of describing the
syntactic structure of a DSL.&nbsp; Once you have a grammar, it's
tedious work to turn it into a handwritten parser, and tedious work
should be done by a computer!</p>

<p>A parser generator uses this grammar file to generate a
parser.&nbsp; The parser can be updated merely by updating the grammar
and regenerating.&nbsp; The generated parser can use efficient
techniques that would be hard to build and maintain by hand.<br>
</p>

<p>The purpose of this section is not to show or teach you how to
build your own Parser Generator but to introduce you to them and what
they offer you. &nbsp;Parser Generators are common tools that take
your grammar file (for example, BNF) to generate a parser.</p>

<p>In computer science, a parser generator or a compiler-compiler
or compiler generator is a
tool that creates a parser, interpreter, or compiler from some form of
formal description of a language and machine. The earliest and still
most common form of compiler-compiler is a parser generator, whose
input is a grammar (usually in BNF) of a programming language, and
whose generated output is the source code of a parser often used as a
component of a compiler.</p>

<p>The usualy way to work with a Parser Generator is to write a
grammar file. &nbsp;This file will likely use a particular form of
BNF used by that Parser Generator. &nbsp;There is not
standardization here so if you change your parser generator you will
likely have to rewrite your grammar.</p>

<p>Once you have a grammar, the usualy route is to use the parser
generator to generate a parser. &nbsp;Most parser generators use
code generation which allows them&nbsp;to output to different host
languages. &nbsp;There is no reason why a parser generator
shouldn't be able to read a grammar file at runtime and interpret it,
then build a parser combinator (more in next section).</p>

<p>Generally, you treat the generated code as black box and don't
delve into it. &nbsp;It is, however, &nbsp;useful to follow
what the parser is doing - particularly if you are trying to debug your
grammar. &nbsp;In this case, there is an advantage in the parser
generator using an algorithm that is easier to follo, such as
generating a Recursive Descent Parser.</p>

<p>The greatest advantage to using a parser generator is that it
provides
an explicit grammar to define the syntactic structure of th elanguage
you're processing. &nbsp;This is the key advantage of using a DSL.
&nbsp;Since
parser generators are primarily designed to handle complicated
languages, they also give you much more features and power than you
would get by writing your own parser. &nbsp;These features may
require some
effort to learn, you can usually start with a simple set and work your
way up from there. Parser Generators may provide good error handling
and diagnostics which can make a big difference when trying to figure
out why your grammar isn't doing whay you think it should.
</p>

<p>In the last lecture we will look at ANTLR parser generator.
&nbsp;We would recommend ANTLR to anyone who wants to explore
parser generators. &nbsp;ANTLR outputs to a variety of host
languages; ANTLR is a mature tool with excellent documentation; ANTLR
is free and easy to download and install.</p>

<h3>Variants</h3>

<p>A typical parser generator associates executable code with
each of
the rules of the grammar that should be executed when these rules are
applied by the parser. These pieces of code are sometimes referred to
as semantic action routines since they define the semantics of the
syntactic structure that is analyzed by the parser. Depending upon the
type of parser that should be generated, these routines may construct a
parse tree (or abstract syntax tree), or generate executable code
directly.<br>
<br>
One of the earliest (1964), surprisingly powerful, versions of
compiler-compilers is MetaII, which accepted grammars and code
generation rules, and is able to compile itself and other languages.<br>
<br>
Some experimental compiler-compilers take as input a formal description
of programming language semantics, typically using denotational
semantics. This approach is often called 'semantics-based compiling',
and was pioneered by Peter Mosses' Semantic Implementation System (SIS)
in 1978. However, both the generated compiler and the code it
produced were inefficient in time and space. No production compilers
are currently built in this way, but research continues.<br>
<br>
The Production Quality Compiler-Compiler project at Carnegie-Mellon
University does not formalize semantics, but does have a semi-formal
framework for machine description.<br>
<br>
Compiler-compilers exist in many flavors, including bottom-up rewrite
machine generators (see <a href="http://jburg.sourceforge.net/">JBurg</a>)
used to tile syntax trees according to a
rewrite grammar for code generation, and attribute grammar parser
generators (e.g. ANTLR can be used for simultaneous type checking,
constant propagation, and more during the parsing stage).<br>
</p>

<h3>History</h3>

<p>The first compiler-compiler to use that name was written by
Tony
Brooker in 1960 and was used to create compilers for the Atlas computer
at the University of Manchester, including the Atlas Autocode compiler.
However it was rather different from modern compiler-compilers, and
today would probably be described as being somewhere between a highly
customisable generic compiler and an extensible-syntax language. The
name 'compiler-compiler' was far more appropriate for Brooker's system
than it is for most modern compiler-compilers, which are more
accurately described as parser generators. It is almost certain that
the "Compiler Compiler" name has entered common use due to Yacc rather
than Brooker's work being remembered.<br>
<br>
Other examples of parser generators in the yacc vein are ANTLR, Coco/R,
CUP, GNU bison, Eli, FSL, SableCC and JavaCC.</p>

<h2>Parser Combinator</h2>

In functional programming, a parser combinator is a higher-order
function which accepts several parsers as input and returns a new
parser as its output. In this context, a parser is a function accepting
strings as input and returning some structure as output, typically a
parse tree or a set of indices representing locations in the string
where parsing stopped successfully. Parser combinators enable a
recursive descent parsing strategy which facilitates modular piecewise
construction and testing. This parsing technique is called combinatory
parsing.<br>

<p>Parsers built using combinators are straightforward to
construct,
&#8216;readable&#8217;, modular, well-structured and easily maintainable. They have
been used extensively in the prototyping of compilers and processors
for domain-specific languages such as natural language interfaces to
databases, where complex and varied semantic actions are closely
integrated with syntactic processing. In 1989, Richard Frost and John
Launchbury demonstrated[1] use of parser combinators to construct
natural language interpreters. Graham Hutton also used higher-order
functions for basic parsing in 1992.[2] S.D. Swierstra also exhibited
the practical aspects of parser combinators in 2001.[3] In 2008, Frost,
Hafiz and Callaghan described[4] a set of parser combinators in Haskell
that solve the long-standing problem of accommodating left recursion,
and work as a complete top-down parsing tool in polynomial time and
space.<br>
<br>
</p>

<h3>Basic idea</h3>

<p>In functional programming, parser combinators can be used to
combine
basic parsers to construct parsers for more complex rules. For example,
a production rule of a context-free grammar (CFG) may have one or more
&#8216;alternatives&#8217; and each alternative may consist of a sequence of
non-terminal(s) and/or terminal(s), or the alternative may consist of a
single non-terminal or terminal or the empty string. If a simple parser
is available for each of these alternatives, a parser combinator can be
used to combine each of these parsers, returning a new parser which can
recognise any or all of the alternatives.<br>
<br>
A parser combinator can take the form of an infix operator, used to
&#8216;glue&#8217; different parsers to form a complete rule. Parser combinators
thereby enable parsers to be defined in an embedded style, in code
which is similar in structure to the rules of the grammar. As such,
implementations can be thought of as executable specifications with all
of the associated advantages.<br>
<br>
</p>

<h3>The combinators</h3>

<p>To keep the discussion relatively straightforward, we discuss
parser
combinators in terms of recognizers only. If the input string is of
length #input and its members are accessed through an index j, a
recognizer is a parser which returns, as output, a set of indices
representing positions at which the parser successfully finished
recognizing a sequence of tokens that began at position j. An empty
result set indicates that the recognizer failed to recognize any
sequence beginning at index j. A non-empty result set indicates the
recognizer ends at different positions successfully.<br>
<br>
&nbsp;&nbsp;&nbsp; The empty recognizer recognizes the
empty string.
This parser always succeeds, returning a singleton set containing the
current position:<br>
<br>
&nbsp;&nbsp;&nbsp; empty(j) = \{j\}<br>
<br>
&nbsp;&nbsp;&nbsp; A recognizer term &#8217;x&#8217; recognizes the
terminal x. If
the token at position j in the input string is x, this parser returns a
singleton set containing j + 1; otherwise, it returns the empty set.<br>
<br>
&nbsp;&nbsp;&nbsp; term(x,j) = \begin{cases} \left \{
\right \}, &amp;
j \geq \#input\\ \left \{ j+1 \right \}, &amp; j^{th} \mbox{
element of
} input=x\\ \left \{ \right \}, &amp; \mbox{otherwise} \end{cases} <br>
<br>
Note that there may be multiple distinct ways to parse a string while
finishing at the same index: this indicates an ambiguous grammar.
Simple recognizers do not acknowledge these ambiguities; each possible
finishing index is listed only once in the result set. For a more
complete set of results, a more complicated object such as a parse tree
must be returned.<br>
<br>
Following the definitions of two basic recognizers p and q, we can
define two major parser combinators for alternative and sequencing:<br>
<br>
&nbsp;&nbsp;&nbsp; The &#8216;alternative&#8217; parser combinator,
&lt;+&gt;,
applies both of the recognizers on the same input position j and sums
up the results returned by both of the recognizers, which is eventually
returned as the final result. It is used as an infix operator between p
and q as follows:<br>
<br>
&nbsp;&nbsp;&nbsp; (p \quad &lt;\!+\!&gt; \quad
q)(j) = p(j) \cup q(j) <br>
<br>
&nbsp;&nbsp;&nbsp; The sequencing of recognizers is done
with the
&lt;*&gt; parser combinator. Like &lt;+&gt;, it is used
as an infix
operator between p and q. But it applies the first recognizer p to the
input position j, and if there is any successful result of this
application, then the second recognizer q is applied to every element
of the result set returned by the first recognizer. &lt;*&gt;
ultimately returns the union of these applications of q.<br>
<br>
&nbsp;&nbsp;&nbsp; (p \quad &lt;*\!&gt; \quad q)(j)
= \bigcup \{ q(k) : k \in p(j) \} <br>
<br>
<br>
</p>

<h3>Examples</h3>

<p>Consider a highly ambiguous context-free grammar, s ::= &#8216;x&#8217; s
s | e.
Using the combinators defined earlier, we can modularly define
executable notations of this grammar in a modern functional language
(e.g. Haskell) as s = term &#8216;x&#8217; &lt;*&gt; s &lt;*&gt; s
&lt;+&gt; empty.
When the recognizer s is applied on an input sequence xxxxx at position
1, according to the above definitions it would return a result set
{5,4,3,2}.<br>
<br>
</p>

<h3>Shortcomings and solutions</h3>

<p>Parser combinators, like all recursive descent parsers, are
not
limited to the context-free grammars and thus do no global search for
ambiguities in the LL(k) parsing Firstk and Followk sets. Thus,
ambiguities are not known until run-time if and until the input
triggers them. In such cases, the recursive descent parser defaults
(perhaps unknown to the grammar designer) to one of the possible
ambiguous paths, resulting in semantic confusion (aliasing) in the use
of the language. This leads to bugs by users of ambiguous programming
languages, which are not reported at compile-time, and which are
introduced not by human error, but by ambiguous grammar. The only
solution which eliminates these bugs is to remove the ambiguities and
use a context-free grammar.<br>
<br>
The simple implementations of parser combinators have some
shortcomings, which are common in top-down parsing. Naïve combinatory
parsing requires exponential time and space when parsing an ambiguous
context-free grammar. In 1996, Frost and Szydlowski[5] demonstrated how
memoization can be used with parser combinators to reduce the time
complexity to polynomial. Later Frost used monads[6] to construct the
combinators for systematic and correct threading of memo-table
throughout the computation.<br>
<br>
Like any top-down recursive descent parsing, the conventional parser
combinators (like the combinators described above) will not terminate
while processing a left-recursive grammar (e.g. s ::= s
&lt;*&gt; term
&#8216;x&#8217;|empty). A recognition algorithm[7] that accommodates ambiguous
grammars with direct left-recursive rules is described by Frost and
Hafiz in 2006. The algorithm curtails the otherwise ever-growing
left-recursive parse by imposing depth restrictions. That algorithm was
extended[8] to a complete parsing algorithm to accommodate indirect as
well as direct left-recursion in polynomial time, and to generate
compact polynomial-size representations of the potentially exponential
number of parse trees for highly ambiguous grammars by Frost, Hafiz and
Callaghan in 2007. This extended algorithm accommodates indirect left
recursion by comparing its &#8216;computed context&#8217; with &#8216;current context&#8217;.
The same authors also described[4] their implementation of a set of
parser combinators written in the Haskell programming language based on
the same algorithm. The X-SAIGA site has more about the algorithms and
implementation details.<br>
</p>

<p><br>
</p>

<p>*********************************<br>
</p>

<p>
The simplest of options that you have when designing an external DSL is
when you have a custom syntax that you've developed a parser
for.&nbsp;
The parsing engine first lexicalizes the input stream, converting them
to recognizable tokens.&nbsp; These tokens are also known as the
terminals of the grammar.&nbsp; The tokens are then fed on to the
production rules and parsed as valid sentences of the grammar.<br>
<br>
The parsing infrastructure is the only processor that does everything
required to process the input DSL script and generate the necessary
output.<br>
<br>
<br>
<br>
<br>
<br>
</p>

<p>A
language is just a set of valid sentences. &nbsp;Every language
appllication will have a parser (recognizer) component. &nbsp;The
whole
point to writing a grammar or Semantic Model is so you
can&nbsp;build a
programthat recognizes sentences in that language. </p>

<p>Language
applications can be complicated and with anything complicated we want
to break them down into bite-size chuncks. &nbsp;The goal is to get
the
components to fit together in a multistage pipeline that analyzes or
manipulates an input stream. &nbsp;The pipeline gradually converts
an
input sentence (valid input sentence) to a handy internal data
structure or translates it to a sentence in another language.</p>

<img style="width: 779px; height: 223px;" alt="Multi-stage Pipeline" src="images/Pipeline.jpg">
<p>The
basic idea is the reader recognizes the input and builds an
intermediate representation (IR) that feeds the rest of the
application. &nbsp;The intermedeiate stages form the semantic
analyzer
which figures out what the input means. &nbsp;At the end, a
generator
emits output based upon the IR and what the application learned in the
intermediate stages.</p>

<p>There are four broad application catagories to the pipeline
described above:</p>

<ol>

  <li>Reader
- A reader builds a data structure from one or more input streams.
&nbsp;The input streams are usually text but can be binary data as
well. &nbsp;Examples include configuration file readers, program
analysis tools such as a method cross-reference tool, and class file
loaders.</li>
  <li>Generator - A generator walks an internal data
structure and emits output. &nbsp;Examples include
object-to-relational
database mapping tools, object serializers, source code generators, and
web page generators.</li>
  <li>Translator or Rewriter - A translator
reads text or binary input and emits output conforming to the same or a
different language. &nbsp;It is essentially a combined reader and
generator. &nbsp;Examples include translators from extinct
programming
languages to modern languages, wiki to HTML translators, refactorers,
profilers that instrument code, log file report generators, pretty
printers, and macro preprocessors. &nbsp;Some translators, such as
assemblers and compilers, are so common that they warrant their own
subcategories.</li>
  <li>Interpreter - An interpreter reads, decodes, and
executes instructions. &nbsp;Interpreters range from simple
calculators
and POP protocol servers all the way up to programming language
implementations such as those for Java, Ruby, and Python.</li>
</ol>

<h3>Parsing Input Sentences - Readers</h3>

The most basic reader component and probably something most of you have
already done in some form, most likely with XML. &nbsp;So here are
a couple of short examples with of parsing an XML file with Python.<br>

<br>

This example uses Document object model:<br>

<br>

<code><br>
&nbsp;&nbsp;from xml.dom import minidom, Node<br>
&nbsp;&nbsp;def scanNode(node, level = 0):<br>
&nbsp;&nbsp;&nbsp;&nbsp;msg = node.__class__.__name__<br>
&nbsp;&nbsp;&nbsp;&nbsp;if node.nodeType ==
Node.ELEMENT_NODE:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;msg += ",
tag: " + node.tagName<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(" " * level * 4, msg)<br>
&nbsp;&nbsp;&nbsp;&nbsp;if node.hasChildNodes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for child
in node.childNodes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scanNode(child,
level + 1)<br>
<br>
&nbsp;&nbsp;doc = minidom.parse('planets.xml')<br>
&nbsp;&nbsp;scanNode(doc)<br>
</code><br>

This example uses tree structure:
<code><br>
<br>
&nbsp;&nbsp;from xml.etree import ElementTree as etree<br>
<br>
&nbsp;&nbsp;def main():<br>
&nbsp;&nbsp;&nbsp;&nbsp;tree =
etree.<span style="font-weight: bold;">parse</span>("planets.xml")<br>
&nbsp;&nbsp;&nbsp;&nbsp;root = tree.getroot()<br>
&nbsp;&nbsp;&nbsp;&nbsp;print("root: ")<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(root)<br>
&nbsp;&nbsp;&nbsp;&nbsp;for child in root:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("
child: ")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(child)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for
grandchild in child:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("
grandchild: ")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(grandchild)<br>
<br>
&nbsp;&nbsp;if __name__ == "__main__":<br>
&nbsp;&nbsp;&nbsp;&nbsp;# Someone is launching this
directly<br>
&nbsp;&nbsp;&nbsp;&nbsp;main()<br>
</code>
<br>

<br>

<h3>Generating Sentences with State Machines</h3>

<h3>The Requirements for Generating Complex Language</h3>

<br>

<br>

<h3>Enforcing Sentence Tree Structure with Pushdown Machines</h3>

<h3>Ambiguous Languages</h3>

<br>

<h3>Vocabulary Symbols Are Structured Too</h3>

<h3>Recognizing Computer Language Sentences</h3>

<br>

<p>A generic template for displaying HTML for a lecture!
</p>

<p>It lacks images, but at least it has style.
And it has links, even if they don't go
anywhere&#8230;
</p>

<p><code>int x = rand()*17;</code>
</p>

<p>There should be more here, but I don't know
what yet.
<!-- Sign and date the page, it's only polite! --></p>

<address>Made 31 August 2011<br>
by D Bartlett.</address>

</body></html>